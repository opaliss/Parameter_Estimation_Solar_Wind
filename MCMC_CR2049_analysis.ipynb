{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ca83f1",
   "metadata": {},
   "source": [
    "Last Modified: March 15th, 2023 [OI]\n",
    "\n",
    "\n",
    "# CR 2049 MCMC results (emcee hammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0141d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import copy\n",
    "import scipy.optimize as opt\n",
    "import emcee\n",
    "import corner\n",
    "import h5py\n",
    "\n",
    "from model_chain import run_chain_of_models_mcmc\n",
    "from sunpy.coordinates.sun import carrington_rotation_time\n",
    "from model_chain import run_chain_of_models, get_ace_date\n",
    "from MCMC_seven_params import *\n",
    "import sunpy.map\n",
    "import astropy.units as u\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527f17fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "font = {'family' : 'serif',\n",
    "        'size'   : 11}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=11) \n",
    "matplotlib.rc('ytick', labelsize=11) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc264e8",
   "metadata": {},
   "source": [
    "* As *a posteriori* parameter dimensionality reduction we set the non-influential parameters to their nomial values, i.e.\n",
    "$$\\delta = 1.75, \\psi = 3.5, \\alpha_{acc} = 0.15, r_h = 50.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d96457",
   "metadata": {},
   "source": [
    "## Bayesian Inference and Markov Chain Monte Carlo for the Ambient Solar Wind Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951a3a7",
   "metadata": {},
   "source": [
    "In this problem, we use Bayesian inference and Markov chain Monte Carlo (MCMC) to find the parameters of\n",
    "a chain of models that are used in space weather operational forecasting. The model and data are connected by \n",
    "$$\n",
    "\\mathbf{y} = \\mathcal{M}(\\mathbf{\\theta}) + \\mathbf{\\eta}\n",
    "$$\n",
    "where $\\mathbf{\\theta} = \\{r_{SS}, v_{0}, v_{1}, \\alpha, \\beta, w, \\gamma\\} \\in \\mathbb{R}^{7}$ is the set of uncertain parameters, $\\mathcal{M}$ is the chain of *PFSS-WSA-HUX* models, $\\mathbf{\\eta}$ represents the discrepancies between the model and data, and is typically assumed to be Gaussian distributed with mean zero and covariance matrix $\\mathbf{\\Sigma}$ (positive definite), and $\\mathbf{y}$ is the the radial velocity in 1hr cadance interval at L1. The radial velocity at L1 is meassured via SWEPAM monior on ACE. \n",
    "\n",
    "* We set $\\mathbf{\\Sigma} = 10 \\cdot \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f507c5c",
   "metadata": {},
   "source": [
    "### Formulation of Bayesian posterior distribution for the model parameters $\\mathbf{\\theta} = [r_{SS}, v_{0}, v_{1}, \\alpha, \\beta, w, \\gamma]$ given the above information.\n",
    "\n",
    "\n",
    "The prior is given by \n",
    "$$\n",
    "\\mathbb{P}(\\theta) = \\mathbb{P}(r_{SS})\\mathbb{P}(v_{0})\\mathbb{P}(v_{1})\\mathbb{P}(\\alpha)\\mathbb{P}(\\beta)\\mathbb{P}(w)\\mathbb{P}(\\gamma) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\frac{1}{22113} & 1.5 \\leq r_{SS} \\leq 4,\\qquad 200 \\leq v_{0} \\leq 400, \\qquad 550 \\leq v_{1} \\leq 950, \\qquad 0.05 \\leq \\alpha \\leq 0.5, \\qquad 1\\leq \\beta \\leq 1.75, \\qquad 0.01 \\leq w \\leq 0.4, \\qquad 0.06 \\leq \\gamma \\leq 0.9 \\\\\n",
    "      0 & \\text{otherwise} \\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "$$\n",
    "since the uncertain parameters $r_{SS}, v_{0}, v_{1}, \\alpha, \\beta, w, \\gamma$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38524a6",
   "metadata": {},
   "source": [
    "The posterior is given by (from Bayes' rule):\n",
    "$$\n",
    "\\mathbb{P}(\\theta | \\mathbf{y}) = \\frac{\\mathbb{P}(\\mathbf{y}|\\theta) \\mathbb{P}(\\theta)}{\\mathbb{P}(\\mathbf{y})} = \\frac{\\mathbb{P}(\\mathbf{y}|\\theta) \\mathbb{P}(\\theta)}{\\int_{\\mathbb{R}^{7}} \\mathbb{P}(\\mathbf{y}|\\theta)\\mathbb{P}(\\theta)\\text{d}\\mathbf{y}} \\propto \\mathbb{P}(\\mathbf{y}|\\theta) \\mathbb{P}(\\theta) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b920c",
   "metadata": {},
   "source": [
    "The likelihood function  updates our belief state by measuring the misfit between the data $\\mathbf{y}$ and model predictions $\\mathcal{M}(\\mathbf{\\theta})$ with a specific parameter set. Since we assume additive Gaussian measurement noise wth zero mean, i.e. $\\eta_j  \\in \\mathcal{N}(\\mathbf{\\mu}= \\mathbf{0},\\mathbf{\\Sigma})$, the measurements are $\\mathbf{y}_{j}$ independent across observations, conditional on the parameters $\\mathbf{\\theta}$, i.e., $\\mathbf{y}_{1}$ and $\\mathbf{y}_{2}$ are independent given $\\mathbf{\\theta}_{1}$ and $\\mathbf{\\theta}_{2}$. Using the independence assumption, we can write the likelihood function as \n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{y}|\\theta) =  \\frac{1}{(2\\pi)^{7/2} |\\Sigma|^{1/2}} \\exp \\left( - \\frac{1}{2}\\left(\\mathbf{y}_{i} - \\mathcal{M}(\\theta_{i})\\right)^{\\top} \\mathbf{\\Sigma}^{-1} \\left(\\mathbf{y}_{i} - \\mathcal{M}(\\theta_{i})\\right)\\right) \\propto  \\exp \\left( -\\frac{1}{2}\\left(\\mathbf{y}_{i} - \\mathcal{M}(\\theta_{i})\\right)^{\\top} \\mathbf{\\Sigma}^{-1} \\left(\\mathbf{y}_{i} - \\mathcal{M}(\\theta_{i})\\right)\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd7848",
   "metadata": {},
   "source": [
    "We initialize the walkers to start close to MAPs obtained from GSA for CR2048, CR2053, and CR2058 with QoI being the MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f4660",
   "metadata": {},
   "source": [
    "### MCMC\n",
    "MCMC is a procedure for generating a random walk in the parameter space that, over time, draws a representative set of samples from the distribution. Each point in a Markov chain $X(t_{i})$ depends only on the position of the previous step $X(t_{i−1})$. \n",
    "\n",
    "The MCMC Metropolis-Hastings (MH)  method involves simultaneously evolving an ensemble of $K$ walkers $S = {X_{k}}$ where the proposal distribution for one walker $k$ is based on the current positions of the $K − 1$ walkers in the complementary ensemble $S_{[k]} = \\{X_{j} , \\forall j\\neq k\\}$.\n",
    "\n",
    "\n",
    "Here, we use a Python implemtation of the affine-invariant ensemble sampler proposed by **Goodman & Weare (2010)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea3a4fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "You must run the sampler with 'store == True' before accessing the results",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m reader \u001b[38;5;241m=\u001b[39m emcee\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mHDFBackend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCMC_results/CR2049.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m nburnin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnburnin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m samples_flatten  \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mget_chain(flat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, discard\u001b[38;5;241m=\u001b[39mnburnin)\n\u001b[1;32m      6\u001b[0m log_prob_samples \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mget_log_prob(flat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, discard\u001b[38;5;241m=\u001b[39mnburnin)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hux-rom/lib/python3.9/site-packages/emcee/backends/backend.py:75\u001b[0m, in \u001b[0;36mBackend.get_chain\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_chain\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the stored chain of MCMC samples\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hux-rom/lib/python3.9/site-packages/emcee/backends/hdf.py:161\u001b[0m, in \u001b[0;36mHDFBackend.get_value\u001b[0;34m(self, name, flat, thin, discard)\u001b[0m\n\u001b[1;32m    159\u001b[0m iteration \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must run the sampler with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore == True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m before accessing the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblobs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m g\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_blobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: You must run the sampler with 'store == True' before accessing the results"
     ]
    }
   ],
   "source": [
    "reader = emcee.backends.HDFBackend(\"MCMC_results/CR2049.h5\")\n",
    "\n",
    "nburnin = 0\n",
    "samples = reader.get_chain(flat=False, discard=nburnin)\n",
    "samples_flatten  = reader.get_chain(flat=True, discard=nburnin)\n",
    "log_prob_samples = reader.get_log_prob(flat=False, discard=nburnin)\n",
    "log_prior_samples = reader.get_blobs(flat=False, discard=nburnin)\n",
    "n_dim = 7\n",
    "n_walkers = np.shape(samples)[1]\n",
    "\n",
    "print(\"flat chain shape: {0}\".format(samples.shape))\n",
    "print(\"flat log prob shape: {0}\".format(log_prob_samples.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b13aa1",
   "metadata": {},
   "source": [
    "### Acceptance ratio: \n",
    "The easiest and simplest indicator that things are going well is the acceptance fraction; it should be in the 0.2 to 0.5 range (there are theorems about this for specific problems; for example **Gelman, Roberts, & Gilks (1996)**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c14a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"acceptance ratio = \", np.mean(reader.accepted/(reader.iteration - nburnin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761b949",
   "metadata": {},
   "source": [
    "### Log-likelihood in parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "_ = ax.plot(log_prob_samples)\n",
    "_ = ax.set_ylabel(\"Log-Likelihood\")\n",
    "_ = ax.set_xlabel(\"Number of iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3bee2",
   "metadata": {},
   "source": [
    "### A corner plot to show correlation (or lack thereof) of the seven model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of labels used for plotting purposes. \n",
    "labels_list = np.array([r\"$r_{SS}$\", r\"$v_{0}$\", r\"$v_{1}$\", r\"$\\alpha$\", r\"$\\beta$\", r\"$w$\", r\"$\\gamma$\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(samples_flatten, labels=labels_list)\n",
    "\n",
    "\n",
    "# Extract the axes\n",
    "axes = np.array(fig.axes).reshape((n_dim, n_dim))\n",
    "\n",
    "for ii in range(n_dim):\n",
    "    ax = axes[ii, ii]\n",
    "    ax.axvline(np.mean(samples[:, :, ii]), label=\"$\\mu$\", color=\"blue\")\n",
    "    ax.axvspan(np.mean(samples[:, :, ii]) - np.std(samples[:, :, ii]), \n",
    "               np.mean(samples[:, :, ii])  + np.std(samples[:, :, ii]), \n",
    "               alpha=0.5,\n",
    "               color=\"lightblue\", \n",
    "               label=\"$\\mu \\pm \\sigma$\")\n",
    "    ax.set_title(\"$\\mu = $\" + str(round(np.mean(samples[:, :, ii]), 2))\n",
    "                 +\"\\n $\\sigma = $\" + str(round(np.std(samples[:,:, ii]), 2)))\n",
    "    ax.legend()\n",
    "    \n",
    "# Loop over the histograms\n",
    "for yi in range(n_dim):\n",
    "    for xi in range(n_dim):\n",
    "        if xi < yi:\n",
    "            ax = axes[yi, xi]\n",
    "            ax.axvline(np.mean(samples[:, :, xi]), color=\"blue\", ls=\"--\")\n",
    "            ax.axhline(np.mean(samples[:, :, yi]), color=\"blue\", ls=\"--\")\n",
    "            ax.scatter(np.mean(samples[:, :, xi]), np.mean(samples[:, :, yi]), color=\"blue\", s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381bb1a",
   "metadata": {},
   "source": [
    "## Diagnostics for MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce45717",
   "metadata": {},
   "source": [
    "**1) Mixing of the Chain (Heuristic):**\n",
    "Good mixing occurs when more of the domain is explored. Bad mixing occurs when the step size is too large and most proposals are rejected, resulting in less space being explored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=7,sharex=True, figsize=(5, 10))\n",
    "for ii in range(7):\n",
    "    ax[ii].plot(samples[:,:, ii], '-', alpha=0.7)\n",
    "    ax[ii].set_ylabel(labels_list[ii])\n",
    "ax[-1].set_xlabel(\"Number of samples\")\n",
    "ax[0].set_title(\"Walkers in Parameter Space\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b36c37",
   "metadata": {},
   "source": [
    "**2) Autocorrelation:** Autocorrelation draws from the fundamental property of a Markov chain: *every Markov chain is memoryless*. We can compute the autocorrelation of states which are $\\ell$ iterations apart as\n",
    "$$\n",
    "R(\\ell) \\equiv \\frac{\\sum_{i=1}^{n-\\ell} (X_{i} -\\bar{X})(X_{i+\\ell} - \\bar{X})}{\\sum_{i=1}^{n} (X_{i} - \\bar{X})^2}\n",
    "$$\n",
    "where $\\bar{X}$ is the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=7, sharey=True, figsize=(15, 3))\n",
    "for jj in range(7):\n",
    "    for ii in np.arange(0, n_walkers, 1):\n",
    "        ax[jj].plot(emcee.autocorr.function_1d(samples[:, ii, jj].T))\n",
    "        ax[jj].set_xlabel(\"$\\ell$\")\n",
    "        ax[jj].hlines(0, xmin=0, xmax=len(samples[:, 0, 0]), color=\"k\", ls=\"--\")\n",
    "    ax[jj].set_title(str(labels_list[jj]))\n",
    "ax[0].set_ylabel(\"$R(\\ell)$\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaed196",
   "metadata": {},
   "source": [
    "**3) Integrated Autocorrelation (IAC) and Effective Sample Size (ESS):** In standard MC, the variance of the estimator is \n",
    "$$\n",
    "\\mathbb{V}\\text{ar} \\left( \\frac{1}{N} \\sum_{i=1}^{N} f(X^{(i)})\\right) = \\frac{\\mathbb{V}\\text{ar}(f(X))}{N}, \\qquad X^{(i)} \\sim i.i.d.\n",
    "$$\n",
    "In MCMC, samples are correlated, so we get additional covariance terms. If we assume that all $X_{i}$ and $X_{j}$ are samples from the target distribution, then we can approximate the variance of the MCMC estimator as \n",
    "$$\n",
    "\\mathbb{V}\\text{ar}\\left(\\bar{f}(X)^{\\text{MCMC}}\\right) = \\mathbb{V}\\text{ar} \\left( \\frac{1}{N} \\sum_{i=1}^{N} f(X^{(i)})\\right) = \\frac{\\mathbb{V}\\text{ar}\\left(f(X) \\right)}{N} \\cdot \\theta\n",
    "$$\n",
    "where the integrated autocorrelation (IAC) $\\theta$ is defined as \n",
    "$$\n",
    "\\theta = \\sum_{\\tau=-\\infty}^{\\infty} \\rho_{f}(\\tau)\n",
    "$$\n",
    "where $\\rho_{f}(\\tau)$ is the normalized autocorrelation function of the stochastic process that generated the chain for $f$. You can estimate $\\rho_{f}(\\tau)$ using a finite chain $\\{ f_{n}\\}_{n=1}^{N}$ as \n",
    "$$\n",
    "\\hat{\\rho}_{f}(\\tau) = \\hat{c}_{f}(\\tau)/\\hat{c}_{f}(0)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat{c}_{f}(\\tau) = \\frac{1}{N-\\tau} \\sum_{n=1}^{N-\\tau} (f_{n} - \\mu_{f}) (f_{n+\\tau} - \\mu_{f})\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\mu_{f} = \\frac{1}{N} \\sum_{n=1}^{N} f_{n}.\n",
    "$$\n",
    "We stimate $\\theta$ using the estimator for $\\rho_{f}(\\tau)$ as \n",
    "$$\n",
    "\\theta(M) = 1 + 2 \\sum_{\\tau=1}^{M} \\hat{\\rho}_{f}(\\tau)\n",
    "$$\n",
    "The effective sample size of MCMC is then $\\frac{N}{\\theta}$ and we want $\\frac{N}{\\theta} \\ll 1$. Essentially, $\\theta$ is the number of steps that are needed fbefore the chain \"forgets\" where it started. Once we estimate $\\theta$ then we can estimate the number of samples that we need to generate to reduce the relative error on the target inegral. Typically, parallel chains longer than $50\\theta$ are often sufficient. \n",
    "\n",
    "The integrated autocorrelation time is an estimate of the number of steps needed in the chain in order to draw independent samples from the target density. A more efficient chain has a shorter autocorrelation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.get_autocorr_time()\n",
    "print(\"More than N=\" + str(np.max(tau*50)) + \" samples in each chain is sufficient.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee5f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210ef7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
